# used for manipulating directory paths
import os

# Scientific and vector computation for python
import numpy as np

# Import regular expressions to process emails
import re

# Plotting library
from matplotlib import pyplot
from mpl_toolkits.mplot3d import Axes3D
import matplotlib as mpl

from IPython.display import HTML, display, clear_output

try:
    pyplot.rcParams["animation.html"] = "jshtml"
except ValueError:
    pyplot.rcParams["animation.html"] = "html5"

# Optimization module in scipy
from scipy import optimize

# will be used to load MATLAB mat datafile format
from scipy.io import loadmat

import utils

class Week8():

  def __init__(self):
    np.random.seed(1)

  def findClosestCentroids(self, X, centroids):
    K = centroids.shape[0]
    idx = np.zeros(X.shape[0], dtype=int)
    for i in np.arange(idx.size):
      J = np.sqrt(np.sum(np.square(X[i] - centroids), axis = 1))
      idx[i] = np.argmin(J)

    return idx
  
  def computeCentroids(self, X, idx, K):
    m, n = X.shape
    centroids = np.zeros((K, n))

    for i in np.arange(K):
      centroids[i] = np.mean(X[idx == i], axis = 0)

    return centroids

  def kMeansInitCentroids(self, X, K):
    m, n = X.shape
    centroids = np.zeros((K, n))

    print(X.shape[0])

    randidx = np.random.permutation(X.shape[0])
     # Take the first K examples as centroids
    centroids = X[randidx[:K], :]

    return centroids
  
  def pca(self, X):
    m, n = X.shape
    U = np.zeros(n)
    S = np.zeros(n)

    Sigma = (1 / m) * (X.T.dot(X))
    U, S, V = np.linalg.svd(Sigma)
    return U, S

  def projectData(self, X, U, K):
    Z = np.zeros((X.shape[0], K))
    Z = np.dot(X, U[:, :K])
    return Z

  def recoverData(self, Z, U, K):
    X_rec = np.zeros((Z.shape[0], U.shape[0]))
    X_rec = Z.dot(U[:, :K].T)
    return X_rec

if __name__ == "__main__":
    K_means_PCA = Week8();

    # Load an example dataset that we will be using
    data = loadmat(os.path.join('Data', 'ex7data2.mat'))
    X = data['X']

    # Select an initial set of centroids
    K = 3   # 3 Centroids
    initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])

    # Find the closest centroids for the examples using the initial_centroids
    idx = K_means_PCA.findClosestCentroids(X, initial_centroids)

    print('Closest centroids for the first 3 examples:')
    print(idx[:3])
    print('(the closest centroids should be 0, 2, 1 respectively)')

    # Compute means based on the closest centroids found in the previous part.
    centroids = K_means_PCA.computeCentroids(X, idx, K)

    print('Centroids computed after initial finding of closest centroids:')
    print(np.around(centroids, 6))
    print('\nThe centroids should be')
    print('   [ 2.428301 3.157924 ]')
    print('   [ 5.813503 2.633656 ]')
    print('   [ 7.119387 3.616684 ]')

    # Load an example dataset
    data = loadmat(os.path.join('Data', 'ex7data2.mat'))

    # Settings for running K-Means
    K = 3
    max_iters = 10

    K_means_PCA.kMeansInitCentroids(X, K)

    # Run K-Means algorithm. The 'true' at the end tells our function to plot
    # the progress of K-Means
    # centroids, idx, anim = utils.runkMeans(X, initial_centroids, K_means_PCA.findClosestCentroids, K_means_PCA.computeCentroids, max_iters, True)
    # print(anim)

    K = 16
    max_iters = 10

    A = mpl.image.imread(os.path.join('Data', 'bird_small.png'))

    A /= 255

    # Reshape the image into an Nx3 matrix where N = number of pixels.
    # Each row will contain the Red, Green and Blue pixel values
    # This gives us our dataset matrix X that we will use K-Means on.
    X = A.reshape(-1, 3)

    initial_centroids = K_means_PCA.kMeansInitCentroids(X, K)

    # Run K-Means
    centroids, idx = utils.runkMeans(X, initial_centroids,
                                     K_means_PCA.findClosestCentroids,
                                     K_means_PCA.computeCentroids,
                                     max_iters)

    # We can now recover the image from the indices (idx) by mapping each pixel
    # (specified by its index in idx) to the centroid value
    # Reshape the recovered image into proper dimensions
    X_recovered = centroids[idx, :].reshape(A.shape)


    # # Display the original image, rescale back by 255
    # fig, ax = pyplot.subplots(1, 2, figsize=(8, 4))
    # ax[0].imshow(A*255)
    # ax[0].set_title('Original')
    # ax[0].grid(False)

    # # Display compressed image, rescale back by 255
    # ax[1].imshow(X_recovered*255)
    # ax[1].set_title('Compressed, with %d colors' % K)
    # ax[1].grid(False)

    # Load the dataset into the variable X 
    data = loadmat(os.path.join('Data', 'ex7data1.mat'))
    X = data['X']

    #  Visualize the example dataset
    pyplot.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=1)
    pyplot.axis([0.5, 6.5, 2, 8])
    pyplot.gca().set_aspect('equal')
    pyplot.grid(False)

    X_norm, mu, sigma = utils.featureNormalize(X)

    U, S = K_means_PCA.pca(X_norm)

    #  Draw the eigenvectors centered at mean of data. These lines show the
    #  directions of maximum variations in the dataset.
    fig, ax = pyplot.subplots()
    ax.plot(X[:, 0], X[:, 1], 'bo', ms=10, mec='k', mew=0.25)

    for i in range(2):
        ax.arrow(mu[0], mu[1], 1.5 * S[i]*U[0, i], 1.5 * S[i]*U[1, i],
                 head_width=0.25, head_length=0.2, fc='k', ec='k', lw=2, zorder=1000)

    ax.axis([0.5, 6.5, 2, 8])
    ax.set_aspect('equal')
    ax.grid(False)

    print('Top eigenvector: U[:, 0] = [{:.6f} {:.6f}]'.format(U[0, 0], U[1, 0]))
    print(' (you should expect to see [-0.707107 -0.707107])')

    #  Project the data onto K = 1 dimension
    K = 1
    Z = K_means_PCA.projectData(X_norm, U, K)
    print('Projection of the first example: {:.6f}'.format(Z[0, 0]))
    print('(this value should be about    : 1.481274)')

    X_rec  = K_means_PCA.recoverData(Z, U, K)
    print('Approximation of the first example: [{:.6f} {:.6f}]'.format(X_rec[0, 0], X_rec[0, 1]))
    print('       (this value should be about  [-1.047419 -1.047419])')

    #  Plot the normalized dataset (returned from featureNormalize)
    fig, ax = pyplot.subplots(figsize=(5, 5))
    ax.plot(X_norm[:, 0], X_norm[:, 1], 'bo', ms=8, mec='b', mew=0.5)
    ax.set_aspect('equal')
    ax.grid(False)
    pyplot.axis([-3, 2.75, -3, 2.75])

    # Draw lines connecting the projected points to the original points
    ax.plot(X_rec[:, 0], X_rec[:, 1], 'ro', mec='r', mew=2, mfc='none')
    for xnorm, xrec in zip(X_norm, X_rec):
        ax.plot([xnorm[0], xrec[0]], [xnorm[1], xrec[1]], '--k', lw=1)

    #  Load Face dataset
    data = loadmat(os.path.join('Data', 'ex7faces.mat'))
    X = data['X']
    #  Display the first 100 faces in the dataset
    utils.displayData(X[:100, :], figsize=(8, 8))

    #  normalize X by subtracting the mean value from each feature
    X_norm, mu, sigma = utils.featureNormalize(X)
    #  Run PCA
    U, S = K_means_PCA.pca(X_norm)
    #  Visualize the top 36 eigenvectors found
    utils.displayData(U[:, :36].T, figsize=(8, 8))

    #  Project images to the eigen space using the top k eigenvectors 
    #  If you are applying a machine learning algorithm 
    K = 100
    Z = K_means_PCA.projectData(X_norm, U, K)

    print('The projected data Z has a shape of: ', Z.shape)

    #  Project images to the eigen space using the top K eigen vectors and 
    #  visualize only using those K dimensions
    #  Compare to the original input, which is also displayed
    K = 100
    X_rec  = K_means_PCA.recoverData(Z, U, K)

    # Display normalized data
    utils.displayData(X_norm[:100, :], figsize=(6, 6))
    pyplot.gcf().suptitle('Original faces')

    # Display reconstructed data from only k eigenfaces
    utils.displayData(X_rec[:100, :], figsize=(6, 6))
    pyplot.gcf().suptitle('Recovered faces')
    pass

    A = mpl.image.imread(os.path.join('Data', 'bird_small.png'))
    A /= 255
    X = A.reshape(-1, 3)

    # perform the K-means clustering again here
    K = 16
    max_iters = 10
    initial_centroids = K_means_PCA.kMeansInitCentroids(X, K)
    centroids, idx = utils.runkMeans(X, initial_centroids,
                                     K_means_PCA.findClosestCentroids,
                                     K_means_PCA.computeCentroids, max_iters)

    #  Sample 1000 random indexes (since working with all the data is
    #  too expensive. If you have a fast computer, you may increase this.
    sel = np.random.choice(X.shape[0], size=1000)

    fig = pyplot.figure(figsize=(6, 6))
    ax = fig.add_subplot(111, projection='3d')

    ax.scatter(X[sel, 0], X[sel, 1], X[sel, 2], cmap='rainbow', c=idx[sel], s=8**2)
    ax.set_title('Pixel dataset plotted in 3D.\nColor shows centroid memberships')

    # Subtract the mean to use PCA
    X_norm, mu, sigma = utils.featureNormalize(X)
    
    # PCA and project the data to 2D
    U, S = K_means_PCA.pca(X_norm)
    Z = K_means_PCA.projectData(X_norm, U, 2)
    
    fig = pyplot.figure(figsize=(6, 6))
    ax = fig.add_subplot(111)
    
    ax.scatter(Z[sel, 0], Z[sel, 1], cmap='rainbow', c=idx[sel], s=64)
    ax.set_title('Pixel dataset plotted in 2D, using PCA for dimensionality reduction')
    ax.grid(False)
    pass